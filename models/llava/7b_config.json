{
    "embedding": ["image_text"],
    "image_text_emb":{
      "vision_encoder":{
        "emb_size": 1024,
        "feedforward_size": 4096,
        "hidden_size": 1024,
        "hidden_act": "gelu_fast",
        "heads_num": 16,
        "layers_num": 24,
        "dropout": 0.1,
        "max_seq_length": 577,
        "embedding": ["patch", "pos"],
        "patch_proj_bias": false,
        "remove_embedding_layernorm": false,
        "remove_transformer_bias": false,
        "rotary_position_embedding": false,
        "encoder": "transformer",
        "feed_forward": "dense",
        "mask": "fully_visible",
        "layernorm_positioning": "pre",
        "layernorm":"normal"
      },
      "projection":{
        "mlp_hidden_size": 4096,
        "num_mlp_layer": 2
      }, 
      "text":{
        "embedding": ["word"]
      }
    },
    "image_height": 336,
    "image_width": 336,
    "patch_size": 14,
    "remove_embedding_combine_layernorm": true,
    "emb_size": 4096,
    "feedforward_size": 11008,
    "hidden_size": 4096,
    "hidden_act": "silu",
    "heads_num": 32,
    "layers_num": 32,
    "dropout": 0.0,
    "data_processor": "llava",
    "max_seq_length": 2048,
    "remove_transformer_bias": true,
    "remove_embedding_layernorm": true,
    "rotary_position_embedding": true,
    "encoder": "transformer",
    "feed_forward": "gated",
    "mask": "causal",
    "layernorm_positioning": "pre",
    "layernorm": "rms",
    "target": ["lm"]
  }
